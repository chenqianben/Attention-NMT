{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考网站\n",
    "https://blog.csdn.net/TinyJian/article/details/88732635"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题(b)\n",
    "model_embeddings.py："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T17:31:11.249141Z",
     "start_time": "2020-04-09T17:31:10.250642Z"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "CS224N 2018-19: Homework 4\n",
    "model_embeddings.py: Embeddings for the NMT model\n",
    "Pencheng Yin <pcyin@cs.cmu.edu>\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "Anand Dhoot <anandd@stanford.edu>\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModelEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Class that converts input words to their embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, vocab):\n",
    "        \"\"\"\n",
    "        Init the Embedding layers.\n",
    "        @param embed_size (int): Embedding size (dimensionality)\n",
    "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
    "                              See vocab.py for documentation.注意：src是source；tgt是target\n",
    "        \"\"\"\n",
    "        super(ModelEmbeddings, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # default values\n",
    "        self.source = None\n",
    "        self.target = None\n",
    "\n",
    "        src_pad_token_idx = vocab.src['<pad>']\n",
    "        tgt_pad_token_idx = vocab.tgt['<pad>']\n",
    "\n",
    "        ### YOUR CODE HERE (~2 Lines)\n",
    "        ### TODO - Initialize the following variables:\n",
    "        ###     self.source (Embedding Layer for source language)\n",
    "        ###     self.target (Embedding Layer for target langauge)\n",
    "        ###\n",
    "        ### Note:\n",
    "        ###     1. `vocab` object contains two vocabularies:\n",
    "        ###            `vocab.src` for source\n",
    "        ###            `vocab.tgt` for target\n",
    "        ###     2. You can get the length of a specific vocabulary by running:\n",
    "        ###             `len(vocab.<specific_vocabulary>)`\n",
    "        ###     3. Remember to include the padding token for the specific vocabulary\n",
    "        ###        when creating your Embedding.\n",
    "        ###\n",
    "        ### Use the following docs to properly initialize these variables:\n",
    "        ###     Embedding Layer:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
    "        self.source = nn.Embedding(len(vocab.src), self.embed_size, padding_idx=src_pad_token_idx)\n",
    "        self.target = nn.Embedding(len(vocab.tgt), self.embed_size, padding_idx=tgt_pad_token_idx)\n",
    "        ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题(c)\n",
    "nmt_model.py："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras中的LSTMcell 和LSTM 有什么区别？\n",
    "LSTMcell是LSTM层的实现单元，固定将LSTMcell作为它的计算单元。而LSTMcell是一个单步的计算单元。\n",
    "- LSTM是一个经常性的层。// LSTM takes a SEQUENCE of inputs x_1,x_2,…,x_T. No need to write a loop to do one pass of backprop through time.\n",
    "- LSTMCell是LSTM层使用的对象（恰好也是一个层），它包含一步的计算逻辑。// LSTMCell takes ONE input x_t. You need to make a loop in order to do one pass of backprop through time.// LSTMCell就是單一時刻下的LSTM， seq_len = 1。\n",
    "\n",
    "### LSTM 和 LSTMcell的理解（非常详细）：\n",
    "https://medium.com/@phoebehuang.pcs04g/建構一個rnn-理解lstm-與-lstmcell的差異與用法-809d0c190610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T17:31:12.363012Z",
     "start_time": "2020-04-09T17:31:12.305166Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import namedtuple\n",
    "from typing import Dict, List, Set, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from model_embeddings import ModelEmbeddings\n",
    "\n",
    "Hypothesis = namedtuple('Hypothesis', ['value', 'score'])\n",
    "\n",
    "\n",
    "class NMT(nn.Module):\n",
    "    \"\"\" Simple Neural Machine Translation Model:\n",
    "        - Bidrectional LSTM Encoder\n",
    "        - Unidirection LSTM Decoder\n",
    "        - Global Attention Model (Luong, et al. 2015)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2):\n",
    "        \"\"\" Init NMT Model.\n",
    "        @param embed_size (int): Embedding size (dimensionality)\n",
    "        @param hidden_size (int): Hidden Size (dimensionality)\n",
    "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
    "                              See vocab.py for documentation.\n",
    "        @param dropout_rate (float): Dropout probability, for attention\n",
    "        \"\"\"\n",
    "        super(NMT, self).__init__()\n",
    "        self.model_embeddings = ModelEmbeddings(embed_size, vocab)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # default values\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.h_projection = None\n",
    "        self.c_projection = None\n",
    "        self.att_projection = None\n",
    "        self.combined_output_projection = None\n",
    "        self.target_vocab_projection = None\n",
    "        self.dropout = None\n",
    "\n",
    "        ### YOUR CODE HERE (~8 Lines)\n",
    "        ### TODO - Initialize the following variables:\n",
    "        ###     self.encoder (Bidirectional LSTM with bias)\n",
    "        ###     self.decoder (LSTM Cell with bias)\n",
    "        ###     self.h_projection (Linear Layer with no bias), called W_{h} in the PDF.\n",
    "        ###     self.c_projection (Linear Layer with no bias), called W_{c} in the PDF.\n",
    "        ###     self.att_projection (Linear Layer with no bias), called W_{attProj} in the PDF.\n",
    "        ###     self.combined_output_projection (Linear Layer with no bias), called W_{u} in the PDF.\n",
    "        ###     self.target_vocab_projection (Linear Layer with no bias), called W_{vocab} in the PDF.\n",
    "        ###     self.dropout (Dropout Layer)\n",
    "        ###\n",
    "        ### Use the following docs to properly initialize these variables:\n",
    "        ###     LSTM:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n",
    "        ###     LSTM Cell:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell\n",
    "        ###     Linear Layer:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
    "        ###     Dropout Layer:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
    "        self.encoder = nn.LSTM(                   # (seq_len,b,embed_size) -> seq_len,b,num_directions*hidden_size) \n",
    "            embed_size,\n",
    "            self.hidden_size,                     # 这个参数相当于hidden_state的特征数，在参考网站中记为h\n",
    "            dropout=self.dropout_rate,\n",
    "            bidirectional = True,\n",
    "        )\n",
    "        \n",
    "        # the input is ybar_t, h_{t-1} and c_{t-1}. the output is h_{t} and c_{t}\n",
    "        # 这里是对decoder的上一个输出(尺寸为hidden_size)和目标句子y的这一个输入词(尺寸为embed_size)的整合\n",
    "        self.decoder = nn.LSTMCell(embed_size + self.hidden_size, self.hidden_size)  \n",
    "        \n",
    "        self.h_projection = nn.Linear(2 * self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.c_projection = nn.Linear(2 * self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.att_projection = nn.Linear(2 * self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.combined_output_projection = nn.Linear(3 * self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.target_vocab_projection = nn.Linear(self.hidden_size, len(self.vocab.tgt), bias=False)\n",
    "        self.dropout = nn.Dropout(p = self.dropout_rate)\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def forward(self, source: List[List[str]],\n",
    "                target: List[List[str]]) -> torch.Tensor:\n",
    "        \"\"\" Take a mini-batch of source and target sentences, compute the log-likelihood of\n",
    "        target sentences under the language models learned by the NMT system.\n",
    "        @param source (List[List[str]]): list of source sentence tokens\n",
    "        @param target (List[List[str]]): list of target sentence tokens, wrapped by `<s>` and `</s>`\n",
    "        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the\n",
    "                                    log-likelihood of generating the gold-standard target sentence for\n",
    "                                    each example in the input batch. Here b = batch size.\n",
    "        \"\"\"\n",
    "        # Compute sentence lengths\n",
    "        source_lengths = [len(s) for s in source]\n",
    "\n",
    "        # Convert list of lists into tensors\n",
    "        source_padded = self.vocab.src.to_input_tensor(source, device=self.device)  # Tensor: (src_len, b)\n",
    "        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)  # Tensor: (tgt_len, b)\n",
    "\n",
    "        ###     Run the network forward:\n",
    "        ###     1. Apply the encoder to `source_padded` by calling `self.encode()`\n",
    "        ###     2. Generate sentence masks for `source_padded` by calling `self.generate_sent_masks()`\n",
    "        ###     3. Apply the decoder to compute combined-output by calling `self.decode()`\n",
    "        ###     4. Compute log probability distribution over the target vocabulary using the\n",
    "        ###        combined_outputs returned by the `self.decode()` function.\n",
    "\n",
    "        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)\n",
    "        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n",
    "        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state,target_padded)\n",
    "        P = F.log_softmax( self.target_vocab_projection(combined_outputs), dim=-1)\n",
    "\n",
    "        # Zero out, probabilities for which we have nothing in the target text\n",
    "        target_masks = (target_padded != self.vocab.tgt['<pad>']).float()\n",
    "\n",
    "        # Compute log probability of generating true target words\n",
    "        target_gold_words_log_prob = torch.gather(\n",
    "            P, index=target_padded[1:].unsqueeze(-1),\n",
    "            dim=-1).squeeze(-1) * target_masks[1:]\n",
    "        scores = target_gold_words_log_prob.sum(dim=0)\n",
    "        return scores\n",
    "\n",
    "    def encode(self, source_padded: torch.Tensor, source_lengths: List[int]\n",
    "               ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\" Apply the encoder to source sentences to obtain encoder hidden states.\n",
    "            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n",
    "        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where\n",
    "                                        b = batch_size, src_len = maximum source sentence length. Note that \n",
    "                                       these have already been sorted in order of longest to shortest sentence.\n",
    "        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch\n",
    "        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, 2*h), where\n",
    "                                        b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
    "        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial\n",
    "                                                hidden state and cell.\n",
    "        \"\"\"\n",
    "        enc_hiddens, dec_init_state = None, None\n",
    "\n",
    "        ### YOUR CODE HERE (~ 8 Lines)\n",
    "        ### TODO:\n",
    "        ###     1. Construct Tensor `X` of source sentences with shape (src_len, b, e) using the source model embeddings.\n",
    "        ###         src_len = maximum source sentence length, b = batch size, e = embedding size. Note\n",
    "        ###         that there is no initial hidden state or cell for the decoder.\n",
    "        ###     2. Compute `enc_hiddens`, `last_hidden`, `last_cell` by applying the encoder to `X`.\n",
    "        ###         - Before you can apply the encoder, you need to apply the `pack_padded_sequence` function to X.\n",
    "        ###         - After you apply the encoder, you need to apply the `pad_packed_sequence` function to enc_hiddens.\n",
    "        ###         - Note that the shape of the tensor returned by the encoder is (src_len, b, h*2) and we want to\n",
    "        ###           return a tensor of shape (b, src_len, h*2) as `enc_hiddens`.\n",
    "        ###     3. Compute `dec_init_state` = (init_decoder_hidden, init_decoder_cell):\n",
    "        ###         - `init_decoder_hidden`:\n",
    "        ###             `last_hidden` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.\n",
    "        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).\n",
    "        ###             Apply the h_projection layer to this in order to compute init_decoder_hidden.\n",
    "        ###             This is h_0^{dec} in the PDF. Here b = batch size, h = hidden size\n",
    "        ###         - `init_decoder_cell`:\n",
    "        ###             `last_cell` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.\n",
    "        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).\n",
    "        ###             Apply the c_projection layer to this in order to compute init_decoder_cell.\n",
    "        ###             This is c_0^{dec} in the PDF. Here b = batch size, h = hidden size\n",
    "        ###\n",
    "        ### See the following docs, as you may need to use some of the following functions in your implementation:\n",
    "        ###     Pack the padded sequence X before passing to the encoder:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence\n",
    "        ###     Pad the packed sequence, enc_hiddens, returned by the encoder:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence\n",
    "        ###     Tensor Concatenation:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n",
    "        ###     Tensor Permute:\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute\n",
    "\n",
    "        # pack_padded_sequence与pad_packed_sequence的解析：\n",
    "        # https://www.cnblogs.com/sbj123456789/p/9834018.html\n",
    "        # https://blog.csdn.net/lssc4205/article/details/79474735  注意里面的图，相当于：\n",
    "        # 对于整个seq_len长度应该有seq_len个encoding的过程，但是有一些句子长度短，每沿着句子做一下encoding有一些句子就要结束了(长度不够)，\n",
    "        # 这里通过pack规定了特殊的batch_size,每一个特定的t(1=<t<=seq_len)都有一个特定的batch_size，决定这个batch里面的哪些句子还要继续\n",
    "        \n",
    "        # 1. enc_hiddens\n",
    "        X = self.model_embeddings.source(source_padded)                     # (src_len, b) -> (src_len, b, e)\n",
    "        # X(seq_len,b,embed_size) -> enc_hiddens(seq_len,b,2*hidden_size)  with hidden/cell state (2,b,hidden_size)\n",
    "        # 注意！一个nn.LSTM就表示了整个句子的encoding传递过程，因为h和c一直在传递中更新。\n",
    "        # 但是注意，初始的h0和c0都没有提供，表示每一批新的句子来了之后，都要初始h0和c0为0\n",
    "        enc_hiddens, (last_hidden, last_cell) = self.encoder(pack_padded_sequence(X, source_lengths))\n",
    "        enc_hiddens = enc_hiddens.transpose((1,0,2))                        # (seq_len,b,2*hidden_size) -> (b,seq_len,2*hidden_size)\n",
    "        enc_hiddens = pad_packed_sequence(enc_hiddens,batch_first = True)[0]# (b,seq_len,2*hidden_size) 重新pad回来\n",
    "        \n",
    "        # 2. dec_init_state\n",
    "        last_hidden = torch.cat((last_hidden[0, :], last_hidden[1, :]), 1)  # (2,b,hidden_size) -> (b,2*hidden_size)\n",
    "        init_decoder_hidden = self.h_projection(last_hidden)                # (b,2*hidden_size) -> (b,hidden_size)\n",
    "        \n",
    "        last_cell = torch.cat((last_cell[0, :], last_cell[1, :]), 1)        # (2,b,hidden_size) -> (b,2*hidden_size)\n",
    "        init_decoder_cell = self.c_projection(last_cell)                    # (b,2*hidden_size) -> (b,hidden_size)\n",
    "        \n",
    "        dec_init_state = (init_decoder_hidden, init_decoder_cell)\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        return enc_hiddens, dec_init_state\n",
    "\n",
    "    def decode(self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,\n",
    "               dec_init_state: Tuple[torch.Tensor, torch.Tensor],\n",
    "               target_padded: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute combined output vectors for a batch.\n",
    "        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where\n",
    "                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
    "        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where\n",
    "                                     b = batch size, src_len = maximum source sentence length. # 有单词的地方为1,否则为0\n",
    "        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder\n",
    "        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where\n",
    "                                       tgt_len = maximum target sentence length, b = batch size. \n",
    "        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where\n",
    "                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n",
    "        \"\"\"\n",
    "        # Chop of the <END> token for max length sentences.\n",
    "        target_padded = target_padded[:-1]\n",
    "\n",
    "        # Initialize the decoder state (hidden and cell)\n",
    "        dec_state = dec_init_state\n",
    "\n",
    "        # Initialize previous combined output vector o_{t-1} as zero\n",
    "        batch_size = enc_hiddens.size(0)\n",
    "        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)         # (batch_size, hidden_size)\n",
    "\n",
    "        # Initialize a list we will use to collect the combined output o_t on each step\n",
    "        combined_outputs = []\n",
    "\n",
    "        ### YOUR CODE HERE (~9 Lines)\n",
    "        ### TODO:\n",
    "        ###     1. Apply the attention projection layer to `enc_hiddens` to obtain `enc_hiddens_proj`,\n",
    "        ###         which should be shape (b, src_len, h),\n",
    "        ###         where b = batch size, src_len = maximum source length, h = hidden size.\n",
    "        ###         This is applying W_{attProj} to h^enc, as described in the PDF.\n",
    "        ###     2. Construct tensor `Y` of target sentences with shape (tgt_len, b, e) using the target model embeddings.\n",
    "        ###         where tgt_len = maximum target sentence length, b = batch size, e = embedding size.\n",
    "        ###     3. Use the torch.split function to iterate over the time dimension of Y.\n",
    "        ###         Within the loop, this will give you Y_t of shape (1, b, e) where b = batch size, e = embedding size.\n",
    "        ###             - Squeeze Y_t into a tensor of dimension (b, e).\n",
    "        ###             - Construct Ybar_t by concatenating Y_t with o_prev.\n",
    "        ###             - Use the step function to compute the the Decoder's next (cell, state) values\n",
    "        ###               as well as the new combined output o_t.\n",
    "        ###             - Append o_t to combined_outputs\n",
    "        ###             - Update o_prev to the new o_t.\n",
    "        ###     4. Use torch.stack to convert combined_outputs from a list length tgt_len of\n",
    "        ###         tensors shape (b, h), to a single tensor shape (tgt_len, b, h)\n",
    "        ###         where tgt_len = maximum target sentence length, b = batch size, h = hidden size.\n",
    "        ###\n",
    "        ### Note:\n",
    "        ###    - When using the squeeze() function make sure to specify the dimension you want to squeeze\n",
    "        ###      over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n",
    "        ###\n",
    "        ### Use the following docs to implement this functionality:\n",
    "        ###     Zeros Tensor:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.zeros\n",
    "        ###     Tensor Splitting (iteration):\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.split\n",
    "        ###     Tensor Dimension Squeezing:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze\n",
    "        ###     Tensor Concatenation:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n",
    "        ###     Tensor Stacking:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.stack\n",
    "        ### 这里就可以看出为什么encode使用LSTM,但是decode只能用LSTMCell,因为在decode时候每次都需要一个对目标句子输入单词的concatenation\n",
    "        \n",
    "        enc_hiddens_proj = self.att_projection(enc_hiddens)  #1  (b, seq_len,2*hidden_layers) -> (b, seq_len,hidden_layers)\n",
    "\n",
    "        Y = self.model_embeddings.target(target_padded)      #2  (tgt_len, b) -> (tgt_len, b, e)\n",
    "\n",
    "        for Y_t in torch.split(Y, 1,dim=0):                  #3  在维度0进行长度为1的拆分,即每次在维度0取一下, Y_t(1, b, e)\n",
    "            Y_t = torch.squeeze(Y_t)                         # (1, b, e) -> (b, e)\n",
    "            Ybar_t = torch.cat((o_prev, Y_t), dim=1)         # (b,h) + (b,e) -> (b,h+e)\n",
    "            dec_state, o_t, e_t = self.step(Ybar_t, dec_state, enc_hiddens, # 输出 当前(state,cell), combined_output, 以及e_t(只是test时候用)\n",
    "                                            enc_hiddens_proj, enc_masks)\n",
    "            \n",
    "            combined_outputs.append(o_t)\n",
    "            o_prev = o_t\n",
    "\n",
    "        combined_outputs = torch.stack(combined_outputs,dim=0)     # a list of (b,h)  -> (tgt_len, b, h)\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        return combined_outputs\n",
    "\n",
    "    def step(self, Ybar_t: torch.Tensor,\n",
    "             dec_state: Tuple[torch.Tensor, torch.Tensor],\n",
    "             enc_hiddens: torch.Tensor, enc_hiddens_proj: torch.Tensor,\n",
    "             enc_masks: torch.Tensor\n",
    "             ) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\" Compute one forward step of the LSTM decoder, including the attention computation.\n",
    "        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n",
    "                                where b = batch size, e = embedding size, h = hidden size.\n",
    "        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.\n",
    "                First tensor is decoder's prev hidden state, second tensor is decoder's prev cell.\n",
    "        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n",
    "                                    src_len = maximum source length, h = hidden size.\n",
    "        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),\n",
    "                                    where b = batch size, src_len = maximum source length, h = hidden size.\n",
    "        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),\n",
    "                                    where b = batch size, src_len is maximum source length. \n",
    "        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.\n",
    "                First tensor is decoder's new hidden state, second tensor is decoder's new cell.\n",
    "        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.\n",
    "        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.\n",
    "                                Note: You will not use this outside of this function.\n",
    "                                      We are simply returning this value so that we can sanity check\n",
    "                                      your implementation.\n",
    "        \"\"\"\n",
    "\n",
    "        combined_output = None\n",
    "\n",
    "        ### YOUR CODE HERE (~3 Lines)\n",
    "        ### TODO:\n",
    "        ###     1. Apply the decoder to `Ybar_t` and `dec_state`to obtain the new dec_state.\n",
    "        ###     2. Split dec_state into its two parts (dec_hidden, dec_cell)\n",
    "        ###     3. Compute the attention scores e_t, a Tensor shape (b, src_len).\n",
    "        ###        Note: b = batch_size, src_len = maximum source length, h = hidden size.\n",
    "        ###\n",
    "        ###       Hints:\n",
    "        ###         - dec_hidden is shape (b, h) and corresponds to h^dec_t in the PDF (batched)\n",
    "        ###         - enc_hiddens_proj is shape (b, src_len, h) and corresponds to W_{attProj} * h^enc (batched).\n",
    "        ###         - Use batched matrix multiplication (torch.bmm) to compute e_t.\n",
    "        ###         - To get the tensors into the right shapes for bmm, you will need to do some squeezing and unsqueezing.\n",
    "        ###         - When using the squeeze() function make sure to specify the dimension you want to squeeze\n",
    "        ###             over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n",
    "        ###\n",
    "        ### Use the following docs to implement this functionality:\n",
    "        ###     Batch Multiplication:\n",
    "        ###        https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
    "        ###     Tensor Unsqueeze:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.unsqueeze\n",
    "        ###     Tensor Squeeze:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze\n",
    "        dec_state = self.decoder(Ybar_t, dec_state)    # Ybar_t(b, h+e) -> (b, h) with dec_state(b, h)\n",
    "        dec_hidden, dec_cell = dec_state\n",
    "        e_t = torch.squeeze(torch.bmm(enc_hiddens_proj, torch.unsqueeze(dec_hidden,dim=2)), dim=2)  # (b, src_len)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Set e_t to -inf where enc_masks has 1\n",
    "        # enc_mask makes the probability of <paded> approaching 0\n",
    "        if enc_masks is not None:\n",
    "            e_t.data.masked_fill_(enc_masks.byte(), -float('inf'))\n",
    "\n",
    "        ### YOUR CODE HERE (~6 Lines)\n",
    "        ### TODO:\n",
    "        ###     1. Apply softmax to e_t to yield alpha_t\n",
    "        ###     2. Use batched matrix multiplication between alpha_t and enc_hiddens to obtain the\n",
    "        ###         attention output vector, a_t.\n",
    "        #$$     Hints:\n",
    "        ###           - alpha_t is shape (b, src_len)\n",
    "        ###           - enc_hiddens is shape (b, src_len, 2h)\n",
    "        ###           - a_t should be shape (b, 2h)\n",
    "        ###           - You will need to do some squeezing and unsqueezing.\n",
    "        ###     Note: b = batch size, src_len = maximum source length, h = hidden size.\n",
    "        ###\n",
    "        ###     3. Concatenate dec_hidden with a_t to compute tensor U_t\n",
    "        ###     4. Apply the combined output projection layer to U_t to compute tensor V_t\n",
    "        ###     5. Compute tensor O_t by first applying the Tanh function and then the dropout layer.\n",
    "        ###\n",
    "        ### Use the following docs to implement this functionality:\n",
    "        ###     Softmax:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.functional.softmax\n",
    "        ###     Batch Multiplication:\n",
    "        ###        https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
    "        ###     Tensor View:\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "        ###     Tensor Concatenation:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n",
    "        ###     Tanh:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.tanh\n",
    "\n",
    "        alpha_t = F.softmax(e_t, dim=1)  # attention vector (b, src_len)\n",
    "        a_t = torch.squeeze(torch.bmm(torch.unsqueeze(alpha_t, 1), enc_hiddens), 1)  # context vector\n",
    "        U_t = torch.cat((a_t, dec_hidden), 1)            # (b, 2h) + (b, h) -> (b, 3h)\n",
    "        V_t = self.combined_output_projection(U_t)       # (b, 3h) -> (b, h)\n",
    "        O_t = self.dropout(torch.tanh(V_t))              # (b, h)\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        combined_output = O_t\n",
    "        return dec_state, combined_output, e_t\n",
    "\n",
    "    def generate_sent_masks(self, enc_hiddens: torch.Tensor,\n",
    "                            source_lengths: List[int]) -> torch.Tensor:\n",
    "        \"\"\" Generate sentence masks for encoder hidden states.\n",
    "        @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,\n",
    "                                     src_len = max source length, h = hidden size. \n",
    "        @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.\n",
    "        \n",
    "        @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),\n",
    "                                    where src_len = max source length, h = hidden size.\n",
    "        \"\"\"\n",
    "        enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)   # (b, src_len)\n",
    "        for e_id, src_len in enumerate(source_lengths):\n",
    "            enc_masks[e_id, src_len:] = 1\n",
    "        return enc_masks.to(self.device)\n",
    "\n",
    "    def beam_search(self,\n",
    "                    src_sent: List[str],\n",
    "                    beam_size: int = 5,\n",
    "                    max_decoding_time_step: int = 70) -> List[Hypothesis]:\n",
    "        \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n",
    "        @param src_sent (List[str]): a single source sentence (words)\n",
    "        @param beam_size (int): beam size\n",
    "        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n",
    "        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n",
    "                value: List[str]: the decoded target sentence, represented as a list of words\n",
    "                score: float: the log-likelihood of the target sentence\n",
    "        \"\"\"\n",
    "        src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n",
    "\n",
    "        src_encodings, dec_init_vec = self.encode(src_sents_var,\n",
    "                                                  [len(src_sent)])\n",
    "        src_encodings_att_linear = self.att_projection(src_encodings)\n",
    "\n",
    "        h_tm1 = dec_init_vec\n",
    "        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n",
    "\n",
    "        eos_id = self.vocab.tgt['</s>']\n",
    "\n",
    "        hypotheses = [['<s>']]\n",
    "        hyp_scores = torch.zeros(\n",
    "            len(hypotheses), dtype=torch.float, device=self.device)\n",
    "        completed_hypotheses = []\n",
    "\n",
    "        t = 0\n",
    "        while len(completed_hypotheses\n",
    "                  ) < beam_size and t < max_decoding_time_step:\n",
    "            t += 1\n",
    "            hyp_num = len(hypotheses)\n",
    "\n",
    "            exp_src_encodings = src_encodings.expand(hyp_num,\n",
    "                                                     src_encodings.size(1),\n",
    "                                                     src_encodings.size(2))\n",
    "\n",
    "            exp_src_encodings_att_linear = src_encodings_att_linear.expand(\n",
    "                hyp_num, src_encodings_att_linear.size(1),\n",
    "                src_encodings_att_linear.size(2))\n",
    "\n",
    "            y_tm1 = torch.tensor(\n",
    "                [self.vocab.tgt[hyp[-1]] for hyp in hypotheses],\n",
    "                dtype=torch.long,\n",
    "                device=self.device)\n",
    "            y_t_embed = self.model_embeddings.target(y_tm1)\n",
    "\n",
    "            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n",
    "\n",
    "            (h_t, cell_t), att_t, _ = self.step(\n",
    "                x,\n",
    "                h_tm1,\n",
    "                exp_src_encodings,\n",
    "                exp_src_encodings_att_linear,\n",
    "                enc_masks=None)\n",
    "\n",
    "            # log probabilities over target words\n",
    "            log_p_t = F.log_softmax(\n",
    "                self.target_vocab_projection(att_t), dim=-1)\n",
    "\n",
    "            live_hyp_num = beam_size - len(completed_hypotheses)\n",
    "            contiuating_hyp_scores = (\n",
    "                hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n",
    "            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(\n",
    "                contiuating_hyp_scores, k=live_hyp_num)\n",
    "\n",
    "            prev_hyp_ids = top_cand_hyp_pos / len(self.vocab.tgt)\n",
    "            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n",
    "\n",
    "            new_hypotheses = []\n",
    "            live_hyp_ids = []\n",
    "            new_hyp_scores = []\n",
    "\n",
    "            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(\n",
    "                    prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n",
    "                prev_hyp_id = prev_hyp_id.item()\n",
    "                hyp_word_id = hyp_word_id.item()\n",
    "                cand_new_hyp_score = cand_new_hyp_score.item()\n",
    "\n",
    "                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n",
    "                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n",
    "                if hyp_word == '</s>':\n",
    "                    completed_hypotheses.append(\n",
    "                        Hypothesis(\n",
    "                            value=new_hyp_sent[1:-1],\n",
    "                            score=cand_new_hyp_score))\n",
    "                else:\n",
    "                    new_hypotheses.append(new_hyp_sent)\n",
    "                    live_hyp_ids.append(prev_hyp_id)\n",
    "                    new_hyp_scores.append(cand_new_hyp_score)\n",
    "\n",
    "            if len(completed_hypotheses) == beam_size:\n",
    "                break\n",
    "\n",
    "            live_hyp_ids = torch.tensor(\n",
    "                live_hyp_ids, dtype=torch.long, device=self.device)\n",
    "            h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n",
    "            att_tm1 = att_t[live_hyp_ids]\n",
    "\n",
    "            hypotheses = new_hypotheses\n",
    "            hyp_scores = torch.tensor(\n",
    "                new_hyp_scores, dtype=torch.float, device=self.device)\n",
    "\n",
    "        if len(completed_hypotheses) == 0:\n",
    "            completed_hypotheses.append(\n",
    "                Hypothesis(\n",
    "                    value=hypotheses[0][1:], score=hyp_scores[0].item()))\n",
    "\n",
    "        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n",
    "\n",
    "        return completed_hypotheses\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n",
    "        \"\"\"\n",
    "        return self.model_embeddings.source.weight.device\n",
    "\n",
    "    @staticmethod\n",
    "    def load(model_path: str):\n",
    "        \"\"\" Load the model from a file.\n",
    "        @param model_path (str): path to model\n",
    "        \"\"\"\n",
    "        params = torch.load(\n",
    "            model_path, map_location=lambda storage, loc: storage)\n",
    "        args = params['args']\n",
    "        model = NMT(vocab=params['vocab'], **args)\n",
    "        model.load_state_dict(params['state_dict'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def save(self, path: str):\n",
    "        \"\"\" Save the odel to a file.\n",
    "        @param path (str): path to the model\n",
    "        \"\"\"\n",
    "        print('save model parameters to [%s]' % path, file=sys.stderr)\n",
    "\n",
    "        params = {\n",
    "            'args':\n",
    "            dict(\n",
    "                embed_size=self.model_embeddings.embed_size,\n",
    "                hidden_size=self.hidden_size,\n",
    "                dropout_rate=self.dropout_rate),\n",
    "            'vocab':\n",
    "            self.vocab,\n",
    "            'state_dict':\n",
    "            self.state_dict()\n",
    "        }\n",
    "\n",
    "        torch.save(params, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T17:53:43.896854Z",
     "start_time": "2020-04-09T17:53:40.243155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize source vocabulary ..\n",
      "number of word types: 128871, number of word types w/ frequency >= 5: 30985\n",
      "initialize target vocabulary ..\n",
      "number of word types: 172420, number of word types w/ frequency >= 5: 36725\n",
      "generated vocabulary, source 30989 words, target 36727 words\n"
     ]
    }
   ],
   "source": [
    "from utils import read_corpus\n",
    "from vocab import Vocab\n",
    "\n",
    "# load data\n",
    "train_src = r'.\\en_es_data\\train.en'  # 被翻译语言\n",
    "train_tgt = r'.\\en_es_data\\train.es'  # 翻译语言\n",
    "\n",
    "train_data_src = read_corpus(train_src, source='src')\n",
    "train_data_tgt = read_corpus(train_tgt, source='tgt')\n",
    "train_data = list(zip(train_data_src, train_data_tgt))\n",
    "\n",
    "# load vocab\n",
    "vocab = Vocab.build(train_data_src, train_data_tgt, vocab_size=50000, freq_cutoff=5)\n",
    "print('generated vocabulary, source %d words, target %d words' % (len(vocab.src), len(vocab.tgt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T17:55:41.914212Z",
     "start_time": "2020-04-09T17:55:41.774584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# model parameters: 19388946\n"
     ]
    }
   ],
   "source": [
    "model = NMT(embed_size = 256, hidden_size = 50, vocab = vocab, dropout_rate=0.2)\n",
    "print('# model parameters:', sum(param.numel() for param in model.parameters()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
